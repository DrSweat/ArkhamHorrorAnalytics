# Arkham Horror Analytics

Personal (pet) project in which I analyse decks present in ArkhamDB.

I have no pretention. So far the project do simple analytics: how often cards are used in investigators decks.

## Disclaimer

I'm not a developer. I work in the IT industry and I know my way around scripting and automation, but I don't consider myself being a good Python developer. I learn by myself. The code is working, but it's surely not _good_ code. I've started this project mainly because I'm a fan of Arkham Horror The Card Game and I wanted to teach myself a bit of Python (including multithreading).

## How this script works?

Using [ArkhamDB](https://arkhamdb.com/) public [API](https://arkhamdb.com/api/), the Python script get the content of each deck. Then, it parses the deck slots (cards) and build JSON data structure containing the relationship between the cards themselves and between cards and specific investigator.

All data files generated and used by the script are added to ```db/``` or ```output/``` folders.

### db folder

This folder contains data that is mostly static and should not change over time.

| Subfolder | Content | GitIgnore |
| --------- | ------- | --------- |
| card      | Cache of the cards from ArkhamDB. | Yes _(Note 1)_ |
| decklist  | Cache of the decks from ArkhamDB. | Yes _(Note 1)_ |
| other     | Other static or generated files used by the script. | No |

_Note 1: Content of these folder is not included in this repo. Cache is built by the script._

### output folder

The files contains in these folder will get updated every time you run the script. It contains all files generated by the script. This content is constantly evolving.

| Subfolder | Content |
| --------- | ------- |
| html      | HTML rendered files. |
| json      | JSON generated by the script. |
| text      | Text files generated by the script. |

## How do I use it?

It helps me select cards when I build or upgrade an investigator deck. Being a casual player, I don't always have the intuition to select the best cards and I found ArkhamDB very helpful, but lacking this affinity data and statistics.

There are very obvious cards and the data is skew around older investigators and expansion since I don't do any normalization or adjustment to the data.

## Features

- Fetch and cache decks from ArkhamDB.
- Fetch and cache cards from ArkhamDB.
- Deduplication of cards and decks.
- Create a list of duplicate decks (hash).
- Create Investigators affinity files: JSON, text and HTML.
- Create cards affinity files in JSON.

### On my to do list

- Improve/optimize Python code by myself or with the help of the community.
- Improve the card parsing to remove any spoiler cards (reject Campaign/Scenario specific cards).
- Generating text and html file for cards affinity.
- Remove illegal decks (Some illegal decks are present in ArkhamDB, this can skew the data).
- Create a new output file will generic stats like how many decks and how long it took to run the script.

## Known issues

- There's surely bugs.
- Not a lot of features.
- Sub-optimal Python code.
- When a deck is deleted, the script always tries to fetch it (Example: deck id 8).
- Once the script reaches the last known deck on ArkhamDB, it keeps trying. I should find a way to detect we reached the last deck of ArkhamDB.

## Side Notes

The current version of the script, with everything in cache (not initial run) will take less than 15 minutes on my system (Ubuntu on Windows WSL).
